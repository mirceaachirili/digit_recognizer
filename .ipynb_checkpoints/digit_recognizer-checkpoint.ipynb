{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read datasets into Pandas DataFrames\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "holdout = pd.read_csv('test.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into train (80%), dev (10%), test(10%) sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dev_test_split(dataset):\n",
    "    '''\n",
    "    Splits dataset into train(80%), developement(10%) and test sets(10%).\n",
    "    \n",
    "    Argument:\n",
    "    dataset -- Pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    train_X, train_Y -- training set and respective labels\n",
    "    dev_X, dev_Y -- developement set and respective labels\n",
    "    test_X, test_Y -- test set and respective labels\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(2)\n",
    "    shuffled_idx = np.random.permutation(dataset.shape[0])\n",
    "    \n",
    "    dataset = dataset.iloc[shuffled_idx]\n",
    "\n",
    "    split_1 = int(0.8 * dataset.shape[0])\n",
    "    split_2 = int(0.9 * dataset.shape[0])\n",
    "    \n",
    "    train = dataset.iloc[:split_1]\n",
    "    dev = dataset.iloc[split_1:split_2]\n",
    "    test = dataset.iloc[split_2:]\n",
    "    \n",
    "    X = dataset.iloc[:, 1:].values.astype('float32')\n",
    "    Y = dataset.iloc[:, 0].values.astype('int32')\n",
    "    \n",
    "    train_X, train_Y = train.iloc[:, 1:].values.astype('float32'), train.iloc[:, 0].values.astype('int32')\n",
    "    dev_X, dev_Y = dev.iloc[:, 1:].values.astype('float32'), dev.iloc[:, 0].values.astype('int32')\n",
    "    test_X, test_Y = test.iloc[:, 1:].values.astype('float32'), test.iloc[:, 0].values.astype('int32')\n",
    "    \n",
    "    return train_X, train_Y, dev_X, dev_Y, test_X, test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "\n",
    "train_X, train_Y, dev_X, dev_Y, test_X, test_Y = train_dev_test_split(train)\n",
    "\n",
    "holdout = holdout.values.astype('float32')\n",
    "\n",
    "# Standardize the datasets\n",
    "\n",
    "for dataset in [train_X, dev_X, test_X]:   \n",
    "    dataset = dataset / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training example label: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vizualize training example\n",
    "\n",
    "num_px = np.sqrt(train_X.shape[1]).astype('int32')\n",
    "plt.imshow(train_X[0].reshape(num_px, num_px))\n",
    "\n",
    "print(\"Training example label:\", train_Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    '''\n",
    "    Converts vector to an array of one hot vectors\n",
    "    \n",
    "    Arguments: \n",
    "    Y -- input vector of shape (number of examples,) \n",
    "    C -- number of classes, integer\n",
    "    \n",
    "    Returns:\n",
    "    one_hot -- array of one hot vectors, of shape (number of examples, C)\n",
    "    '''\n",
    "    one_hot = np.eye(C)[Y.reshape(-1)]\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def relu(z):\n",
    "    '''\n",
    "    Computes the relu function of z\n",
    "    \n",
    "    Arguments:\n",
    "    z -- Scalar or array of any size\n",
    "    \n",
    "    Returns:\n",
    "    a -- maximum of (z, 0)\n",
    "    cache -- storage for z; useful for backpropagation\n",
    "    '''\n",
    "    \n",
    "    a = np.maximum(z, 0)\n",
    "    cache = z\n",
    "    \n",
    "    return a, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "def softmax(z, axis=1):\n",
    "    '''\n",
    "    Computes the softmax function of z\n",
    "    \n",
    "    Arguments:\n",
    "    z -- Array of any size\n",
    "    \n",
    "    Returns:\n",
    "    s -- softmax of z\n",
    "    cache -- storage for z; useful for backpropagation\n",
    "    '''\n",
    "    \n",
    "    e_x = np.exp(z)\n",
    "    sum_e_x = np.sum(e_x, axis)\n",
    "    sum_e_x = sum_e_x[:, np.newaxis]  # adding axis to the array. necessary step to do broadcasting\n",
    "    \n",
    "    s = e_x / sum_e_x\n",
    "    cache = z\n",
    "    \n",
    "    return s, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.84203357, 0.04192238, 0.00208719, 0.11395685]]),\n",
       " array([[ 5,  2, -1,  3]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure the softmax function is working correcty \n",
    "sft_test = np.array([5,2,-1,3]).reshape(1,4)\n",
    "\n",
    "softmax(sft_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l-1], layer_dims[l])\n",
    "                    bl -- bias vector of shape (1, layer_dims[l-1])\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l-1], layer_dims[l]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((1, layer_dims[l]))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l-1], layer_dims[l]))\n",
    "        assert(parameters['b' + str(l)].shape == (1, layer_dims[l]))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters shape \n",
      "\n",
      "W1: (784, 4)\n",
      "b1: (1, 4)\n",
      "W2: (4, 5)\n",
      "b2: (1, 5)\n",
      "W3: (5, 10)\n",
      "b3: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(layer_dims=[784, 4, 5, 10])\n",
    "\n",
    "print('Parameters shape', '\\n')\n",
    "for param in parameters.keys():\n",
    "    print(param + ': ' + str(parameters[param].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (number of examples, size of previous layer)\n",
    "    W -- weights matrix: numpy array of shape (size of previous layer, size of current layer)\n",
    "    b -- bias vector, numpy array of shape (1, size of the current layer)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(A, W) + b\n",
    "    \n",
    "    assert(Z.shape == (A.shape[0], W.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (number of examples, size of previous layer)\n",
    "    W -- weights matrix: numpy array of shape (size of previous layer, size of current layer)\n",
    "    b -- bias vector, numpy array of shape (1, size of the current layer)\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    elif activation == 'softmax':\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = softmax(Z)\n",
    "    \n",
    "    assert(A.shape == (A_prev.shape[0], W.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    '''\n",
    "    Implement forward propagation of the neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X - input array of shape (number of examples, 784)\n",
    "    parameters -- python dictionary containing model parameters \n",
    "    \n",
    "    Returns:\n",
    "    AL -- the output of the last layer's activation function\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    '''\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    # first (L-1) layers\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation='relu')\n",
    "        caches.append(cache)\n",
    "        \n",
    "    # Last layer\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation='softmax')\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (X.shape[0], 10))\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Compute the cost\n",
    "    \n",
    "    Arguments:\n",
    "    A3 -- softmax activation output of L_model_forward(), of shape (number of examples, 10)\n",
    "    Y -- \"true\" labels array, of shape (number of examples, 1)\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = AL.shape[0] # number of examples\n",
    "    \n",
    "    # converting Y into a matrix of one hot vectors of shape (number of examples, 10)\n",
    "    Y_one_hot = convert_to_one_hot(Y, 10)\n",
    "    \n",
    "    loss = -np.sum(np.multiply(Y_one_hot, np.log(AL)), axis=1)\n",
    "    \n",
    "    assert (loss.shape == (m,))\n",
    "    \n",
    "    cost = (1/m) * np.sum(loss)\n",
    "    \n",
    "    return cost  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back-propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    '''\n",
    "    Implement linear backward propagation for layer l.\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- gradient of cost function with respect to the linear output (of current layer l)\n",
    "    cache -- tuple (A_prev, W, b) output from L_model_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev - gradient of cost function with respect to previous activation layer (A_prev)\n",
    "    dW - gradient of cost function with respect to W\n",
    "    db - gradient of cost function with respect to b \n",
    "    '''\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[0]\n",
    "    \n",
    "    dW = 1/m * np.dot(A_prev.T, dZ)\n",
    "    db = 1/m * np.sum(dZ, axis=0, keepdims=True)\n",
    "    dA_prev = np.dot(dZ, W.T)\n",
    "    \n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def linear_activation_backward(dA, cache):\n",
    "    '''\n",
    "    Implement back propagation for the Linear -> Activation layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post activation gradient for layer l\n",
    "    cache -- tuple of values (linear_cache, activation_cache) for computing back propagation efficiently\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev - gradient of cost function with respect to previous activation layer (l-1)\n",
    "    dW - gradient of cost function with respect to W (current layer l)\n",
    "    db - gradient of cost function with respect to b (current layer l)\n",
    "    \n",
    "    '''\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    dZ = relu_backward(dA, activation_cache)\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    '''\n",
    "    Implement backward propagation on whole network\n",
    "    \n",
    "    Arguments:\n",
    "    AL - probability vector, output of forward propagation\n",
    "    Y - true label vector\n",
    "    caches - list of every cache of L_model_forward() \n",
    "    \n",
    "    Returns:\n",
    "    grads - dictionary with gradients i.e. grads[\"dA\" + str(l)] = ... \n",
    "    '''\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches)         # number of layers\n",
    "    m = AL.shape[0]         # number of examples\n",
    "    \n",
    "    Y_one_hot = convert_to_one_hot(Y, 10)\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    \n",
    "    dZL = AL - Y_one_hot\n",
    "    current_cache = caches[L-1]\n",
    "    \n",
    "    grads['dA' + str(L-1)], grads['dW' + str(L)] , grads['db' + str(L)] = linear_backward(dZL, current_cache[0])\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        grads['dA' + str(l)], grads['dW' + str(l+1)] , grads['db' + str(l+1)] = linear_activation_backward(grads['dA' + str(l+1)], current_cache)\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing model parameters \n",
    "    grads -- python dictionary containing model gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (â‰ˆ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "    ### END CODE HERE ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch gradient descent\n",
    "\n",
    "def NN_model(X, Y, layer_dims, num_iterations, learning_rate, print_cost=False):\n",
    "    '''\n",
    "    Implement a L-layer neural network: LINEAR->RELU->...->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (number examples, num_px)\n",
    "    Y -- true label vector, of shape (number examples, 1)\n",
    "    layer_dims -- list containing input size and each layer size, of lenght (no. layers + 1)\n",
    "    num_iterations -- number of iterations of the optimization step\n",
    "    learning_rate -- for gradient descent\n",
    "    print_cost -- if True, prints cost every 20 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by NN model, useful for predictions\n",
    "    '''\n",
    "    \n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        # Forward propagation\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Cost function\n",
    "        cost = compute_cost(AL, Y)\n",
    "        \n",
    "        # Back propagation. Computing the parameters' gradients with repect to the cost functions\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    "        # Updating parameters using gradient descent\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if print_cost and i % 20 == 0:\n",
    "            print('cost after iteration {}: {}'.format(i, cost))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-batch gradient descent\n",
    "\n",
    "def init_mini_batches(X, Y, mini_batch_size, seed = 0):\n",
    "    '''\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (number of examples, input size)\n",
    "    Y -- true label vector of shape (number of examples, 1)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of mini-batches (mini_batch_X, mini_batch_Y)\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    #Shuffle X and Y\n",
    "    permuts = np.random.permutation(m)\n",
    "    X_random = X[permuts, :]\n",
    "    Y_random = Y[permuts]\n",
    "    \n",
    "    # Creating the minibatches\n",
    "    mini_batches = []\n",
    "    complete_minibatches = np.floor(m / mini_batch_size).astype('int')\n",
    "    \n",
    "    # When mini_batch == mini_batch_size\n",
    "    for k in range(complete_minibatches):\n",
    "        mini_batch_X = X_random[(mini_batch_size*k):(mini_batch_size*(k+1)), :]\n",
    "        mini_batch_Y = Y_random[(mini_batch_size*k):(mini_batch_size*(k+1))]\n",
    "        \n",
    "        mini_batches.append((mini_batch_X, mini_batch_Y))\n",
    "    \n",
    "    # When last mini_batch < mini_batch_size\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = X_random[(mini_batch_size * complete_minibatches):, :]\n",
    "        mini_batch_Y = Y_random[(mini_batch_size * complete_minibatches):]\n",
    "        \n",
    "        mini_batches.append((mini_batch_X, mini_batch_Y))\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent with momentum \n",
    "\n",
    "def initialize_momentum(parameters):\n",
    "    '''\n",
    "    Initializes the velocity as a python dictionary with:\n",
    "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
    "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters.\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    \n",
    "    Returns:\n",
    "    v -- python dictionary containing the current velocity.\n",
    "                    v['dW' + str(l)] = velocity of dWl\n",
    "                    v['db' + str(l)] = velocity of dbl\n",
    "    '''\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers\n",
    "    v = {}    # storage for velocity\n",
    "    \n",
    "    # Initialize velocity\n",
    "    for l in range(L):\n",
    "        v['dW' + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n",
    "        v['db' + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)\n",
    "    \n",
    "    return v\n",
    "\n",
    "\n",
    "def update_params_momentum(parameters, grads, v, beta, learning_rate):\n",
    "    '''\n",
    "    Update parameters using Momentum\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients for each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    v -- python dictionary containing the current velocity:\n",
    "                    v['dW' + str(l)] = ...\n",
    "                    v['db' + str(l)] = ...\n",
    "    beta -- the momentum hyperparameter, scalar\n",
    "    learning_rate -- the learning rate, scalar\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    v -- python dictionary containing your updated velocities\n",
    "    '''\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers\n",
    "\n",
    "    # Momentum update\n",
    "    for l in range(L):\n",
    "        # Updating W[l+1] using velocity\n",
    "        v['dW' + str(l+1)] = (beta * v['dW' + str(l+1)]) + (1-beta) * grads['dW' + str(l+1)]\n",
    "        parameters['W' + str(l+1)] = parameters['W' + str(l+1)] - learning_rate * v['dW' + str(l+1)]\n",
    "        \n",
    "        # Updating b[l+1] using velocity\n",
    "        v['db' + str(l+1)] = (beta * v['db' + str(l+1)]) + (1-beta) * grads['db' + str(l+1)]\n",
    "        parameters['b' + str(l+1)] = parameters['b' + str(l+1)] - learning_rate * v['db' + str(l+1)]\n",
    "        \n",
    "    return parameters, v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters):\n",
    "    '''\n",
    "    Initializes v and s as two python dictionaries with:\n",
    "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
    "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters.\n",
    "                    parameters[\"W\" + str(l)] = Wl\n",
    "                    parameters[\"b\" + str(l)] = bl\n",
    "    \n",
    "    Returns: \n",
    "    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n",
    "                    v[\"dW\" + str(l)] = ...\n",
    "                    v[\"db\" + str(l)] = ...\n",
    "    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n",
    "                    s[\"dW\" + str(l)] = ...\n",
    "                    s[\"db\" + str(l)] = ...\n",
    "\n",
    "    '''\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers\n",
    "    v = {}   # exponentially weighted average of the gradient\n",
    "    s = {}   # exponentially weighted average of the squared gradient\n",
    "    \n",
    "    # initialize v and s as array of zeros\n",
    "    for l in range(L):\n",
    "        v['dW' + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n",
    "        v['db' + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)       \n",
    "        s['dW' + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n",
    "        s['db' + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)\n",
    "    \n",
    "    return v, s\n",
    "\n",
    "\n",
    "def update_params_adam(parameters, grads, learning_rate, v, s, t, beta_1, beta_2, epsilon):\n",
    "    '''\n",
    "    Update parameters using Adam\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients for each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "    t -- number of steps taken of Adam \n",
    "    beta1 -- Exponential decay hyperparameter for the first moment estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the second moment estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "    '''\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers\n",
    "    v_corrected = {}\n",
    "    s_corrected = {}\n",
    "    \n",
    "    # update parameters using Adam algorithm\n",
    "\n",
    "    for l in range(L):\n",
    "        # moving average of gradients\n",
    "        v['dW' + str(l+1)] = (beta_1 * v['dW' + str(l+1)]) + (1-beta_1) * grads['dW' + str(l+1)]\n",
    "        v['db' + str(l+1)] = (beta_1 * v['db' + str(l+1)]) + (1-beta_1) * grads['db' + str(l+1)]\n",
    "        \n",
    "        # moving average of gradients with bias correction\n",
    "        v_corrected['dW' + str(l+1)] = v['dW' + str(l+1)] / (1 - np.power(beta_1, t))\n",
    "        v_corrected['db' + str(l+1)] = v['db' + str(l+1)] / (1 - np.power(beta_1, t))\n",
    "        \n",
    "        # moving average of squared gradients\n",
    "        s['dW' + str(l+1)] = (beta_2 * s['dW' + str(l+1)]) + (1-beta_2) * np.power(grads['dW' + str(l+1)], 2)\n",
    "        s['db' + str(l+1)] = (beta_2 * s['db' + str(l+1)]) + (1-beta_2) * np.power(grads['db' + str(l+1)], 2)\n",
    "        \n",
    "        # moving average of squared gradients with bias correction\n",
    "        s_corrected['dW' + str(l+1)] = s['dW' + str(l+1)] / (1 - np.power(beta_2, t))\n",
    "        s_corrected['db' + str(l+1)] = s['db' + str(l+1)] / (1 - np.power(beta_2, t))\n",
    "        \n",
    "        # update parameters W, b\n",
    "        parameters['W' + str(l+1)] = parameters['W' + str(l+1)] - learning_rate * (v_corrected['dW' + str(l+1)] / (np.sqrt(s_corrected['dW' + str(l+1)]) + epsilon))\n",
    "        parameters['b' + str(l+1)] = parameters['b' + str(l+1)] - learning_rate * (v_corrected['db' + str(l+1)] / (np.sqrt(s_corrected['db' + str(l+1)]) + epsilon))\n",
    "        \n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_model_optimizer(X, Y, layer_dims, optimizer, mini_batch_size, num_epochs, beta, \n",
    "                       beta_1, beta_2, epsilon, learning_rate, seed, print_cost=False):\n",
    "    '''\n",
    "    L-layer neural network model which can be run in different optimizer modes.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (number of examples, num_px)\n",
    "    Y -- true \"label\" vector, of shape (number of examples, 1)\n",
    "    optimizer -- one of the following: (gradient descent, momentum, adam)\n",
    "    layers_dims -- python list, containing the size of each layer\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    mini_batch_size -- the size of a mini batch\n",
    "    beta -- Momentum hyperparameter\n",
    "    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "    num_epochs -- number of epochs\n",
    "    print_cost -- True to print the cost every 20 epochs\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    '''\n",
    "    \n",
    "    m = X.shape[0] # training examples\n",
    "    # initialize parameters\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    \n",
    "    # initialize optimizer\n",
    "    if optimizer == 'gd':\n",
    "        pass\n",
    "    elif optimizer == 'momentum':\n",
    "        v = initialize_momentum(parameters)\n",
    "    elif optimizer == 'adam':\n",
    "        v, s = initialize_adam(parameters)\n",
    "    \n",
    "    costs = []\n",
    "    t = 0 # Initialize Adam counter\n",
    "    \n",
    "    # Running gradient descent over num_epochs    \n",
    "    for i in range(num_epochs):\n",
    "        # initialize mini_batches\n",
    "        seed += 1\n",
    "        mini_batches = init_mini_batches(X, Y, mini_batch_size, seed) \n",
    "        \n",
    "        total_cost = 0\n",
    "        \n",
    "        for minibatch in mini_batches:\n",
    "            # Select minibatches\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "            \n",
    "            # Forward propagation\n",
    "            AL, caches = L_model_forward(minibatch_X, parameters)\n",
    "\n",
    "            # Cost function\n",
    "            total_cost += (compute_cost(AL, minibatch_Y) * AL.shape[0])\n",
    "\n",
    "            # Back propagation. Computing the parameters' gradients with repect to the cost functions\n",
    "            grads = L_model_backward(AL, minibatch_Y, caches)\n",
    "\n",
    "            # Updating parameters using gradient descent\n",
    "            if optimizer == 'gd':\n",
    "                parameters = update_parameters(parameters, grads, learning_rate)\n",
    "            elif optimizer == 'momentum':\n",
    "                parameters, v = update_params_momentum(parameters, grads, v, beta, learning_rate)\n",
    "            elif optimizer == 'adam':\n",
    "                t += 1 # Increment Adam counter\n",
    "                parameters, v, s = update_params_adam(parameters, grads, learning_rate, v, s, t, beta_1, beta_2, epsilon=1e-8)\n",
    "            \n",
    "        avg_cost = total_cost / m\n",
    "        \n",
    "        if print_cost and i % 20 == 0:\n",
    "            print('Cost epoch {}: {}'.format(i, avg_cost))\n",
    "        if print_cost:\n",
    "            costs.append(avg_cost)\n",
    "            \n",
    "    # Plot average cost over epochs\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('Cost')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.xlim(0, num_epochs)\n",
    "    plt.title('Learning rate: ' + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model using momentum optimizer\n",
    "updated_params_momentum = NN_model_optimizer(train_X, train_Y, layer_dims=[784, 6, 6, 10], optimizer='momentum', \n",
    "                    mini_batch_size=512, num_epochs=200, beta=0.9, beta_1=0.9, beta_2=0.999, \n",
    "                    epsilon=1e-8, learning_rate=0.02, seed=0, print_cost=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model using Adam optimizer\n",
    "updated_params_adam = NN_model_optimizer(train_X, train_Y, layer_dims=[784, 100, 100, 10], optimizer='adam', \n",
    "                    mini_batch_size=512, num_epochs=200, beta=0.9, beta_1=0.9, beta_2=0.999, \n",
    "                    epsilon=1e-8, learning_rate=0.00008, seed=0, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model using batch gradient descent\n",
    "params_updated = NN_model(train_X, train_Y, [784, 4, 4, 10], 1000, learning_rate = 0.0005, print_cost=True)\n",
    "params_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    '''\n",
    "    Converts vector to an array of one hot vectors\n",
    "    \n",
    "    Arguments: \n",
    "    parameters -- input vector of shape (number of examples,) \n",
    "    C -- number of classes, integer\n",
    "    \n",
    "    Returns:\n",
    "    one_hot -- array of one hot vectors, of shape (number of examples, C)\n",
    "    '''\n",
    "    \n",
    "    AL, caches = L_model_forward(X, parameters)\n",
    "    predictions = np.argmax(AL, axis=1)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate training set accuracy\n",
    "\n",
    "train_predict = predict(updated_params_adam, train_X)\n",
    "accuracy = train_predict[train_predict == train_Y].shape[0] * 100 / train_Y.shape[0]\n",
    "\n",
    "print(\"Training accuracy: \", round(accuracy, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate development set accuracy\n",
    "\n",
    "dev_predict = predict(updated_params_adam, dev_X)\n",
    "dev_accuracy = dev_predict[dev_predict == dev_Y].shape[0] * 100 / dev_Y.shape[0]\n",
    "\n",
    "print(\"Dev accuracy: \", round(dev_accuracy, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate test set accuracy\n",
    "\n",
    "test_predict = predict(updated_params_adam, test_X)\n",
    "test_accuracy = test_predict[test_predict == test_Y].shape[0] * 100 / test_Y.shape[0]\n",
    "\n",
    "print(\"Test accuracy: \", round(test_accuracy, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save parameters to file\n",
    "\n",
    "np.save('params_004_1000.npy', params_updated) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on holdout set\n",
    "\n",
    "predictions = predict(uptdated_params_adam, holdout)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test set predictions for Kaggle submission\n",
    "idx = range(1, 28001)\n",
    "submission_df = {\"ImageId\": idx,\n",
    "                 \"Label\": predictions}\n",
    "submission = pd.DataFrame(submission_df)\n",
    "\n",
    "submission.to_csv(\"submission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
