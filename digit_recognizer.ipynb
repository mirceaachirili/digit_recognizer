{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n",
       "0       0  ...         0         0         0         0         0         0   \n",
       "1       0  ...         0         0         0         0         0         0   \n",
       "2       0  ...         0         0         0         0         0         0   \n",
       "3       0  ...         0         0         0         0         0         0   \n",
       "4       0  ...         0         0         0         0         0         0   \n",
       "\n",
       "   pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0  \n",
       "1         0         0         0         0  \n",
       "2         0         0         0         0  \n",
       "3         0         0         0         0  \n",
       "4         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read datasets into Pandas DataFrames\n",
    "\n",
    "train = pd.read_csv('train.csv')\n",
    "holdout = pd.read_csv('test.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split dataset into train (80%), dev (10%), test(10%) sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dev_test_split(dataset):\n",
    "    '''\n",
    "    Splits dataset into train(80%), developement(10%) and test sets(10%).\n",
    "    \n",
    "    Argument:\n",
    "    dataset -- Pandas DataFrame\n",
    "    \n",
    "    Returns:\n",
    "    train_X, train_Y -- training set and respective labels\n",
    "    dev_X, dev_Y -- developement set and respective labels\n",
    "    test_X, test_Y -- test set and respective labels\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(2)\n",
    "    shuffled_idx = np.random.permutation(dataset.shape[0])\n",
    "    \n",
    "    dataset = dataset.iloc[shuffled_idx]\n",
    "\n",
    "    split_1 = int(0.8 * dataset.shape[0])\n",
    "    split_2 = int(0.9 * dataset.shape[0])\n",
    "    \n",
    "    train = dataset.iloc[:split_1]\n",
    "    dev = dataset.iloc[split_1:split_2]\n",
    "    test = dataset.iloc[split_2:]\n",
    "    \n",
    "    X = dataset.iloc[:, 1:].values.astype('float32')\n",
    "    Y = dataset.iloc[:, 0].values.astype('int32')\n",
    "    \n",
    "    train_X, train_Y = train.iloc[:, 1:].values.astype('float32'), train.iloc[:, 0].values.astype('int32')\n",
    "    dev_X, dev_Y = dev.iloc[:, 1:].values.astype('float32'), dev.iloc[:, 0].values.astype('int32')\n",
    "    test_X, test_Y = test.iloc[:, 1:].values.astype('float32'), test.iloc[:, 0].values.astype('int32')\n",
    "    \n",
    "    return train_X, train_Y, dev_X, dev_Y, test_X, test_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "\n",
    "train_X, train_Y, dev_X, dev_Y, test_X, test_Y = train_dev_test_split(train)\n",
    "\n",
    "holdout = holdout.values.astype('float32')\n",
    "\n",
    "# Standardize the datasets\n",
    "\n",
    "for dataset in [train_X, dev_X, test_X]:   \n",
    "    dataset = dataset / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training example label: 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vizualize training example\n",
    "\n",
    "num_px = np.sqrt(train_X.shape[1]).astype('int32')\n",
    "plt.imshow(train_X[0].reshape(num_px, num_px))\n",
    "\n",
    "print(\"Training example label:\", train_Y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    '''\n",
    "    Converts vector to an array of one hot vectors\n",
    "    \n",
    "    Arguments: \n",
    "    Y -- input vector of shape (number of examples,) \n",
    "    C -- number of classes, integer\n",
    "    \n",
    "    Returns:\n",
    "    one_hot -- array of one hot vectors, of shape (number of examples, C)\n",
    "    '''\n",
    "    one_hot = np.eye(C)[Y.reshape(-1)]\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def relu(z):\n",
    "    '''\n",
    "    Computes the relu function of z\n",
    "    \n",
    "    Arguments:\n",
    "    z -- Scalar or array of any size\n",
    "    \n",
    "    Returns:\n",
    "    a -- maximum of (z, 0)\n",
    "    cache -- storage for z; useful for backpropagation\n",
    "    '''\n",
    "    \n",
    "    a = np.maximum(z, 0)\n",
    "    cache = z\n",
    "    \n",
    "    return a, cache\n",
    "\n",
    "\n",
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ\n",
    "\n",
    "\n",
    "def softmax(z, axis=1):\n",
    "    '''\n",
    "    Computes the softmax function of z\n",
    "    \n",
    "    Arguments:\n",
    "    z -- Array of any size\n",
    "    \n",
    "    Returns:\n",
    "    s -- softmax of z\n",
    "    cache -- storage for z; useful for backpropagation\n",
    "    '''\n",
    "    \n",
    "    e_x = np.exp(z)\n",
    "    sum_e_x = np.sum(e_x, axis)\n",
    "    sum_e_x = sum_e_x[:, np.newaxis]  # adding axis to the array. necessary step to do broadcasting\n",
    "    \n",
    "    s = e_x / sum_e_x\n",
    "    cache = z\n",
    "    \n",
    "    return s, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.84203357, 0.04192238, 0.00208719, 0.11395685]]),\n",
       " array([[ 5,  2, -1,  3]]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure the softmax function is working correcty \n",
    "sft_test = np.array([5,2,-1,3]).reshape(1,4)\n",
    "\n",
    "softmax(sft_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l-1], layer_dims[l])\n",
    "                    bl -- bias vector of shape (1, layer_dims[l-1])\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(layer_dims)\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l-1], layer_dims[l]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((1, layer_dims[l]))\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l-1], layer_dims[l]))\n",
    "        assert(parameters['b' + str(l)].shape == (1, layer_dims[l]))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters shape \n",
      "\n",
      "W1: (784, 4)\n",
      "b1: (1, 4)\n",
      "W2: (4, 5)\n",
      "b2: (1, 5)\n",
      "W3: (5, 10)\n",
      "b3: (1, 10)\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(layer_dims=[784, 4, 5, 10])\n",
    "\n",
    "print('Parameters shape', '\\n')\n",
    "for param in parameters.keys():\n",
    "    print(param + ': ' + str(parameters[param].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (number of examples, size of previous layer)\n",
    "    W -- weights matrix: numpy array of shape (size of previous layer, size of current layer)\n",
    "    b -- bias vector, numpy array of shape (1, size of the current layer)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(A, W) + b\n",
    "    \n",
    "    assert(Z.shape == (A.shape[0], W.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache\n",
    "\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (number of examples, size of previous layer)\n",
    "    W -- weights matrix: numpy array of shape (size of previous layer, size of current layer)\n",
    "    b -- bias vector, numpy array of shape (1, size of the current layer)\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == 'relu':\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    elif activation == 'softmax':\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = softmax(Z)\n",
    "    \n",
    "    assert(A.shape == (A_prev.shape[0], W.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    '''\n",
    "    Implement forward propagation of the neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X - input array of shape (number of examples, 784)\n",
    "    parameters -- python dictionary containing model parameters \n",
    "    \n",
    "    Returns:\n",
    "    AL -- the output of the last layer's activation function\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    '''\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    # first (L-1) layers\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation='relu')\n",
    "        caches.append(cache)\n",
    "        \n",
    "    # Last layer\n",
    "    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation='softmax')\n",
    "    caches.append(cache)\n",
    "    \n",
    "    assert(AL.shape == (X.shape[0], 10))\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Compute the cost\n",
    "    \n",
    "    Arguments:\n",
    "    A3 -- softmax activation output of L_model_forward(), of shape (number of examples, 10)\n",
    "    Y -- \"true\" labels array, of shape (number of examples, 1)\n",
    "    \n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = AL.shape[0] # number of examples\n",
    "    \n",
    "    # converting Y into a matrix of one hot vectors of shape (number of examples, 10)\n",
    "    Y_one_hot = convert_to_one_hot(Y, 10)\n",
    "    \n",
    "    loss = -np.sum(np.multiply(Y_one_hot, np.log(AL)), axis=1)\n",
    "    \n",
    "    assert (loss.shape == (m,))\n",
    "    \n",
    "    cost = (1/m) * np.sum(loss)\n",
    "    \n",
    "    return cost  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back-propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    '''\n",
    "    Implement linear backward propagation for layer l.\n",
    "    \n",
    "    Arguments:\n",
    "    dZ -- gradient of cost function with respect to the linear output (of current layer l)\n",
    "    cache -- tuple (A_prev, W, b) output from L_model_forward()\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev - gradient of cost function with respect to previous activation layer (A_prev)\n",
    "    dW - gradient of cost function with respect to W\n",
    "    db - gradient of cost function with respect to b \n",
    "    '''\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[0]\n",
    "    \n",
    "    dW = 1/m * np.dot(A_prev.T, dZ)\n",
    "    db = 1/m * np.sum(dZ, axis=0, keepdims=True)\n",
    "    dA_prev = np.dot(dZ, W.T)\n",
    "    \n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    \n",
    "    return dA_prev, dW, db\n",
    "\n",
    "def linear_activation_backward(dA, cache):\n",
    "    '''\n",
    "    Implement back propagation for the Linear -> Activation layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post activation gradient for layer l\n",
    "    cache -- tuple of values (linear_cache, activation_cache) for computing back propagation efficiently\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev - gradient of cost function with respect to previous activation layer (l-1)\n",
    "    dW - gradient of cost function with respect to W (current layer l)\n",
    "    db - gradient of cost function with respect to b (current layer l)\n",
    "    \n",
    "    '''\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    dZ = relu_backward(dA, activation_cache)\n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    '''\n",
    "    Implement backward propagation on whole network\n",
    "    \n",
    "    Arguments:\n",
    "    AL - probability vector, output of forward propagation\n",
    "    Y - true label vector\n",
    "    caches - list of every cache of L_model_forward() \n",
    "    \n",
    "    Returns:\n",
    "    grads - dictionary with gradients i.e. grads[\"dA\" + str(l)] = ... \n",
    "    '''\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches)         # number of layers\n",
    "    m = AL.shape[0]         # number of examples\n",
    "    \n",
    "    Y_one_hot = convert_to_one_hot(Y, 10)\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    \n",
    "    dZL = AL - Y_one_hot\n",
    "    current_cache = caches[L-1]\n",
    "    \n",
    "    grads['dA' + str(L-1)], grads['dW' + str(L)] , grads['db' + str(L)] = linear_backward(dZL, current_cache[0])\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        grads['dA' + str(l)], grads['dW' + str(l+1)] , grads['db' + str(l+1)] = linear_activation_backward(grads['dA' + str(l+1)], current_cache)\n",
    "        \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing model parameters \n",
    "    grads -- python dictionary containing model gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "    ### END CODE HERE ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch gradient descent\n",
    "\n",
    "def NN_model(X, Y, layer_dims, num_iterations, learning_rate, print_cost=False):\n",
    "    '''\n",
    "    Implement a L-layer neural network: LINEAR->RELU->...->LINEAR->SOFTMAX.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (number examples, num_px)\n",
    "    Y -- true label vector, of shape (number examples, 1)\n",
    "    layer_dims -- list containing input size and each layer size, of lenght (no. layers + 1)\n",
    "    num_iterations -- number of iterations of the optimization step\n",
    "    learning_rate -- for gradient descent\n",
    "    print_cost -- if True, prints cost every 20 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by NN model, useful for predictions\n",
    "    '''\n",
    "    \n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        # Forward propagation\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Cost function\n",
    "        cost = compute_cost(AL, Y)\n",
    "        \n",
    "        # Back propagation. Computing the parameters' gradients with repect to the cost functions\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    "        # Updating parameters using gradient descent\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        if print_cost and i % 20 == 0:\n",
    "            print('cost after iteration {}: {}'.format(i, cost))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-batch gradient descent\n",
    "\n",
    "def init_mini_batches(X, Y, mini_batch_size, seed = 0):\n",
    "    '''\n",
    "    Creates a list of random minibatches from (X, Y)\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (number of examples, input size)\n",
    "    Y -- true label vector of shape (number of examples, 1)\n",
    "    mini_batch_size -- size of the mini-batches, integer\n",
    "    \n",
    "    Returns:\n",
    "    mini_batches -- list of mini-batches (mini_batch_X, mini_batch_Y)\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    #Shuffle X and Y\n",
    "    permuts = np.random.permutation(m)\n",
    "    X_random = X[permuts, :]\n",
    "    Y_random = Y[permuts]\n",
    "    \n",
    "    # Creating the minibatches\n",
    "    mini_batches = []\n",
    "    complete_minibatches = np.floor(m / mini_batch_size).astype('int')\n",
    "    \n",
    "    # When mini_batch == mini_batch_size\n",
    "    for k in range(complete_minibatches):\n",
    "        mini_batch_X = X_random[(mini_batch_size*k):(mini_batch_size*(k+1)), :]\n",
    "        mini_batch_Y = Y_random[(mini_batch_size*k):(mini_batch_size*(k+1))]\n",
    "        \n",
    "        mini_batches.append((mini_batch_X, mini_batch_Y))\n",
    "    \n",
    "    # When last mini_batch < mini_batch_size\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = X_random[(mini_batch_size * complete_minibatches):, :]\n",
    "        mini_batch_Y = Y_random[(mini_batch_size * complete_minibatches):]\n",
    "        \n",
    "        mini_batches.append((mini_batch_X, mini_batch_Y))\n",
    "    \n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent with momentum \n",
    "\n",
    "def initialize_momentum(parameters):\n",
    "    '''\n",
    "    Initializes the velocity as a python dictionary with:\n",
    "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
    "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters.\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    \n",
    "    Returns:\n",
    "    v -- python dictionary containing the current velocity.\n",
    "                    v['dW' + str(l)] = velocity of dWl\n",
    "                    v['db' + str(l)] = velocity of dbl\n",
    "    '''\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers\n",
    "    v = {}    # storage for velocity\n",
    "    \n",
    "    # Initialize velocity\n",
    "    for l in range(L):\n",
    "        v['dW' + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n",
    "        v['db' + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)\n",
    "    \n",
    "    return v\n",
    "\n",
    "\n",
    "def update_params_momentum(parameters, grads, v, beta, learning_rate):\n",
    "    '''\n",
    "    Update parameters using Momentum\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients for each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    v -- python dictionary containing the current velocity:\n",
    "                    v['dW' + str(l)] = ...\n",
    "                    v['db' + str(l)] = ...\n",
    "    beta -- the momentum hyperparameter, scalar\n",
    "    learning_rate -- the learning rate, scalar\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    v -- python dictionary containing your updated velocities\n",
    "    '''\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers\n",
    "\n",
    "    # Momentum update\n",
    "    for l in range(L):\n",
    "        # Updating W[l+1] using velocity\n",
    "        v['dW' + str(l+1)] = (beta * v['dW' + str(l+1)]) + (1-beta) * grads['dW' + str(l+1)]\n",
    "        parameters['W' + str(l+1)] = parameters['W' + str(l+1)] - learning_rate * v['dW' + str(l+1)]\n",
    "        \n",
    "        # Updating b[l+1] using velocity\n",
    "        v['db' + str(l+1)] = (beta * v['db' + str(l+1)]) + (1-beta) * grads['db' + str(l+1)]\n",
    "        parameters['b' + str(l+1)] = parameters['b' + str(l+1)] - learning_rate * v['db' + str(l+1)]\n",
    "        \n",
    "    return parameters, v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters):\n",
    "    '''\n",
    "    Initializes v and s as two python dictionaries with:\n",
    "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
    "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters.\n",
    "                    parameters[\"W\" + str(l)] = Wl\n",
    "                    parameters[\"b\" + str(l)] = bl\n",
    "    \n",
    "    Returns: \n",
    "    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n",
    "                    v[\"dW\" + str(l)] = ...\n",
    "                    v[\"db\" + str(l)] = ...\n",
    "    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n",
    "                    s[\"dW\" + str(l)] = ...\n",
    "                    s[\"db\" + str(l)] = ...\n",
    "\n",
    "    '''\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers\n",
    "    v = {}   # exponentially weighted average of the gradient\n",
    "    s = {}   # exponentially weighted average of the squared gradient\n",
    "    \n",
    "    # initialize v and s as array of zeros\n",
    "    for l in range(L):\n",
    "        v['dW' + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n",
    "        v['db' + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)       \n",
    "        s['dW' + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n",
    "        s['db' + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)\n",
    "    \n",
    "    return v, s\n",
    "\n",
    "\n",
    "def update_params_adam(parameters, grads, learning_rate, v, s, t, beta_1, beta_2, epsilon):\n",
    "    '''\n",
    "    Update parameters using Adam\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    parameters['W' + str(l)] = Wl\n",
    "                    parameters['b' + str(l)] = bl\n",
    "    grads -- python dictionary containing your gradients for each parameters:\n",
    "                    grads['dW' + str(l)] = dWl\n",
    "                    grads['db' + str(l)] = dbl\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "    t -- number of steps taken of Adam \n",
    "    beta1 -- Exponential decay hyperparameter for the first moment estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the second moment estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
    "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
    "    '''\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers\n",
    "    v_corrected = {}\n",
    "    s_corrected = {}\n",
    "    \n",
    "    # update parameters using Adam algorithm\n",
    "\n",
    "    for l in range(L):\n",
    "        # moving average of gradients\n",
    "        v['dW' + str(l+1)] = (beta_1 * v['dW' + str(l+1)]) + (1-beta_1) * grads['dW' + str(l+1)]\n",
    "        v['db' + str(l+1)] = (beta_1 * v['db' + str(l+1)]) + (1-beta_1) * grads['db' + str(l+1)]\n",
    "        \n",
    "        # moving average of gradients with bias correction\n",
    "        v_corrected['dW' + str(l+1)] = v['dW' + str(l+1)] / (1 - np.power(beta_1, t))\n",
    "        v_corrected['db' + str(l+1)] = v['db' + str(l+1)] / (1 - np.power(beta_1, t))\n",
    "        \n",
    "        # moving average of squared gradients\n",
    "        s['dW' + str(l+1)] = (beta_2 * s['dW' + str(l+1)]) + (1-beta_2) * np.power(grads['dW' + str(l+1)], 2)\n",
    "        s['db' + str(l+1)] = (beta_2 * s['db' + str(l+1)]) + (1-beta_2) * np.power(grads['db' + str(l+1)], 2)\n",
    "        \n",
    "        # moving average of squared gradients with bias correction\n",
    "        s_corrected['dW' + str(l+1)] = s['dW' + str(l+1)] / (1 - np.power(beta_2, t))\n",
    "        s_corrected['db' + str(l+1)] = s['db' + str(l+1)] / (1 - np.power(beta_2, t))\n",
    "        \n",
    "        # update parameters W, b\n",
    "        parameters['W' + str(l+1)] = parameters['W' + str(l+1)] - learning_rate * (v_corrected['dW' + str(l+1)] / (np.sqrt(s_corrected['dW' + str(l+1)]) + epsilon))\n",
    "        parameters['b' + str(l+1)] = parameters['b' + str(l+1)] - learning_rate * (v_corrected['db' + str(l+1)] / (np.sqrt(s_corrected['db' + str(l+1)]) + epsilon))\n",
    "        \n",
    "    return parameters, v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NN_model_optimizer(X, Y, layer_dims, optimizer, mini_batch_size, num_epochs, beta, \n",
    "                       beta_1, beta_2, epsilon, learning_rate, seed, print_cost=False):\n",
    "    '''\n",
    "    L-layer neural network model which can be run in different optimizer modes.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (number of examples, num_px)\n",
    "    Y -- true \"label\" vector, of shape (number of examples, 1)\n",
    "    optimizer -- one of the following: (gradient descent, momentum, adam)\n",
    "    layers_dims -- python list, containing the size of each layer\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    mini_batch_size -- the size of a mini batch\n",
    "    beta -- Momentum hyperparameter\n",
    "    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "    num_epochs -- number of epochs\n",
    "    print_cost -- True to print the cost every 20 epochs\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    '''\n",
    "    \n",
    "    m = X.shape[0] # training examples\n",
    "    # initialize parameters\n",
    "    parameters = initialize_parameters(layer_dims)\n",
    "    \n",
    "    # initialize optimizer\n",
    "    if optimizer == 'gd':\n",
    "        pass\n",
    "    elif optimizer == 'momentum':\n",
    "        v = initialize_momentum(parameters)\n",
    "    elif optimizer == 'adam':\n",
    "        v, s = initialize_adam(parameters)\n",
    "    \n",
    "    costs = []\n",
    "    t = 0 # Initialize Adam counter\n",
    "    \n",
    "    # Running gradient descent over num_epochs    \n",
    "    for i in range(num_epochs):\n",
    "        # initialize mini_batches\n",
    "        seed += 1\n",
    "        mini_batches = init_mini_batches(X, Y, mini_batch_size, seed) \n",
    "        \n",
    "        total_cost = 0\n",
    "        \n",
    "        for minibatch in mini_batches:\n",
    "            # Select minibatches\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "            \n",
    "            # Forward propagation\n",
    "            AL, caches = L_model_forward(minibatch_X, parameters)\n",
    "\n",
    "            # Cost function\n",
    "            total_cost += (compute_cost(AL, minibatch_Y) * AL.shape[0])\n",
    "\n",
    "            # Back propagation. Computing the parameters' gradients with repect to the cost functions\n",
    "            grads = L_model_backward(AL, minibatch_Y, caches)\n",
    "\n",
    "            # Updating parameters using gradient descent\n",
    "            if optimizer == 'gd':\n",
    "                parameters = update_parameters(parameters, grads, learning_rate)\n",
    "            elif optimizer == 'momentum':\n",
    "                parameters, v = update_params_momentum(parameters, grads, v, beta, learning_rate)\n",
    "            elif optimizer == 'adam':\n",
    "                t += 1 # Increment Adam counter\n",
    "                parameters, v, s = update_params_adam(parameters, grads, learning_rate, v, s, t, beta_1, beta_2, epsilon=1e-8)\n",
    "            \n",
    "        avg_cost = total_cost / m\n",
    "        \n",
    "        if print_cost and i % 20 == 0:\n",
    "            print('Cost epoch {}: {}'.format(i, avg_cost))\n",
    "        if print_cost:\n",
    "            costs.append(avg_cost)\n",
    "            \n",
    "    # Plot average cost over epochs\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('Cost')\n",
    "    plt.xlabel('epochs')\n",
    "    plt.xlim(0, num_epochs)\n",
    "    plt.title('Learning rate: ' + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost epoch 0: 2.2363507900402175\n",
      "Cost epoch 20: 0.32425387683028223\n",
      "Cost epoch 40: 0.2958645575081781\n",
      "Cost epoch 60: 0.2803725071654392\n",
      "Cost epoch 80: 0.2725421303580354\n",
      "Cost epoch 100: 0.27052265275926574\n",
      "Cost epoch 120: 0.26064499762247123\n",
      "Cost epoch 140: 0.26727747236477384\n",
      "Cost epoch 160: 0.2563432532668869\n",
      "Cost epoch 180: 0.26587293928006506\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train model using momentum optimizer\n",
    "updated_params_momentum = NN_model_optimizer(train_X, train_Y, layer_dims=[784, 6, 6, 10], optimizer='momentum', \n",
    "                    mini_batch_size=512, num_epochs=200, beta=0.9, beta_1=0.9, beta_2=0.999, \n",
    "                    epsilon=1e-8, learning_rate=0.02, seed=0, print_cost=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost epoch 0: 1.4665615127550142\n",
      "Cost epoch 20: 0.07492682988281078\n",
      "Cost epoch 40: 0.023248489189304843\n",
      "Cost epoch 60: 0.0060039977148940125\n",
      "Cost epoch 80: 0.001690950251372099\n",
      "Cost epoch 100: 0.0006000874810951491\n",
      "Cost epoch 120: 0.00024142360370219926\n",
      "Cost epoch 140: 0.00010101515819088202\n",
      "Cost epoch 160: 4.4534338847210894e-05\n",
      "Cost epoch 180: 2.0715114815383486e-05\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train model using Adam optimizer\n",
    "updated_params_adam = NN_model_optimizer(train_X, train_Y, layer_dims=[784, 100, 100, 10], optimizer='adam', \n",
    "                    mini_batch_size=512, num_epochs=200, beta=0.9, beta_1=0.9, beta_2=0.999, \n",
    "                    epsilon=1e-8, learning_rate=0.00008, seed=0, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost after iteration 0: 2.3204651522919497\n",
      "cost after iteration 20: 1.4744491125031745\n",
      "cost after iteration 40: 2.215061175833085\n",
      "cost after iteration 60: 0.6919147691711135\n",
      "cost after iteration 80: 0.3720365221335927\n",
      "cost after iteration 100: 0.3064172644432635\n",
      "cost after iteration 120: 0.27632436173106373\n",
      "cost after iteration 140: 0.2998564277310388\n",
      "cost after iteration 160: 0.23184311188152412\n",
      "cost after iteration 180: 0.21513135086352986\n",
      "cost after iteration 200: 0.20102926220576875\n",
      "cost after iteration 220: 0.18903432886453364\n",
      "cost after iteration 240: 0.1822328528166897\n",
      "cost after iteration 260: 0.18592236737193749\n",
      "cost after iteration 280: 0.16201220773344374\n",
      "cost after iteration 300: 0.15367363480166382\n",
      "cost after iteration 320: 0.14643526821101385\n",
      "cost after iteration 340: 0.1399531584072433\n",
      "cost after iteration 360: 0.13400073303136792\n",
      "cost after iteration 380: 0.12849166631322648\n",
      "cost after iteration 400: 0.12335091906541909\n",
      "cost after iteration 420: 0.1185480453253173\n",
      "cost after iteration 440: 0.11403940474022228\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-4efd0c62d165>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Train model using batch gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mparams_updated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_cost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mparams_updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-b3a4eddef1d8>\u001b[0m in \u001b[0;36mNN_model\u001b[0;34m(X, Y, layer_dims, num_iterations, learning_rate, print_cost)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Back propagation. Computing the parameters' gradients with repect to the cost functions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_model_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;31m# Updating parameters using gradient descent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-09617b99937f>\u001b[0m in \u001b[0;36mL_model_backward\u001b[0;34m(AL, Y, caches)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mcurrent_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcaches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dA'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dW'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'db'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_activation_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dA'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-a85181800285>\u001b[0m in \u001b[0;36mlinear_activation_backward\u001b[0;34m(dA, cache)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mdZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelu_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mdA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_cache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-a85181800285>\u001b[0m in \u001b[0;36mlinear_backward\u001b[0;34m(dZ, cache)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mdW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mm\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mdA_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train model using batch gradient descent\n",
    "params_updated = NN_model(train_X, train_Y, [784, 100, 100, 10], 1000, learning_rate = 0.01, print_cost=True)\n",
    "params_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    '''\n",
    "    Converts vector to an array of one hot vectors\n",
    "    \n",
    "    Arguments: \n",
    "    parameters -- input vector of shape (number of examples,) \n",
    "    C -- number of classes, integer\n",
    "    \n",
    "    Returns:\n",
    "    one_hot -- array of one hot vectors, of shape (number of examples, C)\n",
    "    '''\n",
    "    \n",
    "    AL, caches = L_model_forward(X, parameters)\n",
    "    predictions = np.argmax(AL, axis=1)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  100.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate training set accuracy\n",
    "\n",
    "train_predict = predict(updated_params_adam, train_X)\n",
    "accuracy = train_predict[train_predict == train_Y].shape[0] * 100 / train_Y.shape[0]\n",
    "\n",
    "print(\"Training accuracy: \", round(accuracy, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev accuracy:  97.29\n"
     ]
    }
   ],
   "source": [
    "# Calculate development set accuracy\n",
    "\n",
    "dev_predict = predict(updated_params_adam, dev_X)\n",
    "dev_accuracy = dev_predict[dev_predict == dev_Y].shape[0] * 100 / dev_Y.shape[0]\n",
    "\n",
    "print(\"Dev accuracy: \", round(dev_accuracy, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  97.17\n"
     ]
    }
   ],
   "source": [
    "# Calculate test set accuracy\n",
    "\n",
    "test_predict = predict(updated_params_adam, test_X)\n",
    "test_accuracy = test_predict[test_predict == test_Y].shape[0] * 100 / test_Y.shape[0]\n",
    "\n",
    "print(\"Test accuracy: \", round(test_accuracy, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save parameters to file\n",
    "\n",
    "np.save('params_004_1000.npy', params_updated) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on holdout set\n",
    "\n",
    "predictions = predict(uptdated_params_adam, holdout)\n",
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save test set predictions for Kaggle submission\n",
    "idx = range(1, 28001)\n",
    "submission_df = {\"ImageId\": idx,\n",
    "                 \"Label\": predictions}\n",
    "submission = pd.DataFrame(submission_df)\n",
    "\n",
    "submission.to_csv(\"submission.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
